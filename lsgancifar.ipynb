{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lsgancifar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZ0uBEC+0dkVDfqPyJ+wuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arnaud-Tuynman/gan_sapienza/blob/main/lsgancifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z7plO7aE0I8"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Importation of necessary modules\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "import numpy as np\n",
        "from numpy import *\n",
        "from numpy.random import *\n",
        "#Keras is a deep-learning framework designed for humans, meaning most of the commands are easily understandable. \n",
        "#The load_data() function allows to download the data located in the indicated file, in this case cifar10\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from skimage.transform import resize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Ghmb4E80r-"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Definition of general variables needed in the code\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "ldim = 100 #This is the dimension that will serve to generate the noise that the generator will use : the latent space\n",
        "epoch=1500 #This is the number of times the training process will happen\n",
        "\n",
        "batch=128 #This is the number of images that we will use for one epoch. \n",
        "#The discriminator will have to judge a batch coming from the dataset and a batch from the generator.\n",
        "#The generator will create two batches of images\n",
        "\n",
        "Image_size=(32,32,3) #Setting the image size of the Cifar10 database : (dimension1, dimension2, black&white(1) or color(3))\n",
        "\n",
        "Optimizer = Adam(lr=0.0002, beta_1=0.5) #Those values are the best ones for an optimizer according to the DCGAN paper\n",
        "Loss = 'mse' #This is the mean-squared error. To compare with the binary crossentropy results we write 'binary-crossentropy' instead\n",
        "Activation = 'linear' #This is the activation function for the LSGAN. If we are using binary crossentropy we change it to 'sigmoid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg5fD2V684CA"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Definition of the discriminator model\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(64, (5,5), strides=(2, 2), padding='same', input_shape=Image_size))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.4))\n",
        "\n",
        "discriminator.add(Conv2D(256, (5,5), strides=(2, 2), padding='same'))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.4))\n",
        "\n",
        "discriminator.add(Conv2D(256, (5,5), strides=(2, 2), padding='same'))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.4))\n",
        "\n",
        "discriminator.add(Flatten())\n",
        "discriminator.add(Dense(1, activation=Activation))\n",
        "\n",
        "discriminator.compile(loss=Loss, optimizer=Optimizer, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWC6db9p86gU"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Definition of the generator model\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "generator = Sequential()\n",
        "\n",
        "generator.add(Dense(256*4*4, input_dim=ldim))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(Reshape((4, 4, 256)))\n",
        " \n",
        "generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        " \n",
        "generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        " \n",
        "generator.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        " \n",
        "generator.add(Conv2D(3, (3,3), activation='tanh', padding='same'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMZR2BHF89FR"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Definition of the gan model\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "#This will be used for generator training\n",
        "#In order to train the generator, we have to put the discriminator as untrainable\n",
        "#The discriminator is already compiled so it will not change its training\n",
        "discriminator.trainable = False \n",
        "\n",
        "#The gan is simply composed of both the generator and the discriminator\n",
        "gan = Sequential()\n",
        "gan.add(generator)\n",
        "gan.add(discriminator)\n",
        "gan.compile(loss=Loss, optimizer=Optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEL9lMUl8_OW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690f285a-8424-4a05-8b26-1d9fe74ac847"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Loading of the Cifar10 database\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "#We don't need the validation data for GANs so we leave an underscore meaning it will not be stored\n",
        "(images, category), (_, _) = load_data()\n",
        "\n",
        "#Cifar10 dataset has 10 classes of images. For improved results we only choose one of these classes.\n",
        "#The classes are labeled with numbers from 0 to 9 in the category value.\n",
        "#Extracting the images from the same category leads to having just one class of images (e.g. for category=7 :horses).\n",
        "index=np.where(category==7)\n",
        "dataset=images[index[0]]\n",
        "\n",
        "#We are shaping the data properly\n",
        "dataset = dataset.astype('float32')\n",
        "dataset = (dataset - 127.5) / 127.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "JYr1LbVc9BJk",
        "outputId": "a48832a0-7c0e-4151-e302-e3b930e19340"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Training of the models\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "#We are making a loop to train the model for as many epoch as stated\n",
        "for i in range(epoch):\n",
        "\n",
        "\n",
        "#In this GAN, the discriminator and the generator are trained at the same pace \n",
        "#which is not necessarily the case for all GANs (example the WGAN)\n",
        "\n",
        "\n",
        "#####Discriminator training#####\n",
        "\n",
        "\t#Choosing real images randomly from the database\n",
        "\timage_index = randint(0, dataset.shape[0], batch)\n",
        "\treal_images = dataset[image_index] #These are the real images\n",
        "\treal_label = ones((batch, 1)) #1 is the label for real images\n",
        "\n",
        "\t#Generating a random noise as input of the generator in order to generate different images\n",
        "\tnoise = randn(ldim * batch)\n",
        "\tnoise = noise.reshape(batch, ldim)\n",
        "\tfake_images = generator.predict(noise) #This is the fake images\n",
        "\tfake_label = zeros((batch, 1)) #0 is the label for fake images\n",
        "\n",
        "\t#Stacking the real and fake images together for training of the discriminator\n",
        "\t#This function creates 1 array from 2 arrays\n",
        "\timages = vstack((real_images, fake_images))\n",
        "\tlabels = vstack((real_label, fake_label))\n",
        " \n",
        "  #The train_on_batch() function runs a gradient update on this batch of data\n",
        "  #The discriminator gets images as input data and has to guess the labels correctly\n",
        "  #This function returns the training loss\n",
        "\tdiscriminator_loss, _ = discriminator.train_on_batch(images, labels)\n",
        "\n",
        "\n",
        "#####Generator training#####\n",
        "\n",
        "\t#Noise generation as input of the generator training\n",
        "\tnoise = randn(ldim * batch * 2)\n",
        "\tnoise = noise.reshape(batch*2, ldim)\n",
        "\t#The generated images should aim to be recognized as real by the discriminator, which will be untrainable during the next step\n",
        "\treal_labelg = ones((batch*2, 1))\n",
        " \n",
        " #We have the noise as input data and the generator will have to create an image that will be classified as real\n",
        "\tgenerator_loss = gan.train_on_batch(noise, real_labelg)\n",
        " \n",
        " #The information of the losses of the discriminator and generator is crucial because it gives us information on the training process\n",
        " #If the values are too high or equal to 0, something went wrong and the training should be restarted\n",
        "\tprint('%d, d=%.3f, g=%.3f' % (i+1, discriminator_loss, generator_loss))\n",
        "\n",
        "\n",
        "#####Samples verification#####\n",
        "\n",
        "\t#We should regularly do some checkings on the training to see if it is going in the correct direction\n",
        "\t#We are saving a file with some generated images for inspection\n",
        "\tif (i+1) % 100 == 0 :\n",
        "\n",
        "\t\t_, accuracy_real = discriminator.evaluate(real_images, real_label, verbose=0) #We evaluate the discriminator on the real images\n",
        "\t\t_, accuracy_fake = discriminator.evaluate(fake_images, fake_label, verbose=0) #We evaluate the discriminator on the fake images\n",
        "\n",
        "\t\t#Those are the results of the accuracy of the discriminator. \n",
        "\t\t#The accuracy on real values should not be equal to 100%, else the learning process becomes saturated.\n",
        "\t\tprint('Accuracy of the discriminator real: %0.f%%, fake: %0.f%%' % (accuracy_real*100, accuracy_fake*100))\n",
        "\t\n",
        "\n",
        "\t\t#Saving some generated images to look at them\n",
        "\t\texamples = (fake_images + 1)/2\n",
        "\t\tn=3 # n*n will be the number of fake images shown\n",
        "\t\tfor k in range(n*n):\n",
        "\t\t\tplt.subplot(n, n, 1 + k)\n",
        "\t\t\tplt.axis('off')\n",
        "\t\t\tplt.imshow(examples[k])\n",
        "\t\tplt.savefig('Fake_images%03d' % (i+1))\n",
        "\t\tplt.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-82d50c13b142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#We are making a loop to train the model for as many epoch as stated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxTrPiQ0lXFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda9c7f5-a9d3-4349-cd17-ab44965016a4"
      },
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "Calculating the Inception Score\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "#We are shaping the last results in order to evaluate their inception score\n",
        "result=((examples+1)*127.5).astype('float32')\n",
        "\n",
        "#InceptionV3() is an image recognition neural network which is used to calculate the inception score\n",
        "model = InceptionV3()\n",
        "\n",
        "#The InceptionV3() function accepts only entry in the shape of (299,299,3), so we need to resize all of our images to this shape\n",
        "data_list = list()\n",
        "for i in result:\n",
        "\tdata = resize(i, (299,299,3))\n",
        "\tdata_list.append(data)\n",
        " \n",
        "result=asarray(data_list)\n",
        "\n",
        "#We need to preprocess the data so that the predict() function from InceptionV3 can work\n",
        "#It gives us a probability distribution that needs to be transformed in order to get the inception score\n",
        "#This probability distribution depends on the number of classes perceived by the algorithm\n",
        "\n",
        "P_yx = model.predict(preprocess_input(result))\n",
        "\n",
        "#This is the probability that each image is in a class. The sum of this probability for an image is 1\n",
        "P_y = P_yx.mean(axis=0)\n",
        "\n",
        "#We are using the Kullback-Leibler divergence with logarithmic probabilities\n",
        "#This will give us the randomness of the probability we obtained; the less random the higher the inception score will be\n",
        "#We need the 'e' value in case the probability is 0, which would cause an infinity error\n",
        "e=1E-16\n",
        "KL = P_yx * (log(P_yx + e) - log(P_y + e))\n",
        "\n",
        "#We are making the sum of the values for each image\n",
        "sum_KL = KL.sum(axis=1)\n",
        "# Averaging on all the images and adding an exponential to counteract the previous logarithm\n",
        "#If the average of the sum is close to zero, it means no structure has been detected and the IS will be close to one\n",
        "IS = exp(mean(sum_KL))\n",
        "\n",
        "print('Inception score : ', IS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inception score :  1.7456135\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}